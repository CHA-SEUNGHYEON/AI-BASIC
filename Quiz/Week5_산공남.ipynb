{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. 가장 먼저 학습 데이터를 준비해보도록 하겠습니다. MNIST 데이터셋을 직접 Load해 봅시다. 데이터셋을 로드하고 DataLoader를 구현해보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader를 이용해 MNIST 데이터셋을 로드해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "root = './data'\n",
    "mnist_train = dset.MNIST(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dset.MNIST(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "#data loader를 직접 구현해보자.\n",
    "train_loader = DataLoader(mnist_train, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.데이터가 준비가 되었다면, 이제 그 데이터를 학습할 모델을 구현할 차례입니다. 그 후 모델 안의 가중치를 초기화시켜보세요. 입력 데이터 형태에 맞도록 linear한 모델을 구성해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 입력의 크기는 28x28입니다.\n",
    "\n",
    "### 여기서 구현하는 linear 모델은 입력이 1차원이기 때문에 입력 차원을 맞춰보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5597, -2.2007,  0.3742,  ...,  1.1078,  0.7588, -0.8797],\n",
       "        [ 1.9275, -0.7699, -0.9300,  ..., -0.7964, -0.7338, -1.2891],\n",
       "        [ 0.2891,  0.2262, -0.4337,  ..., -0.8227, -0.3446,  1.2654],\n",
       "        ...,\n",
       "        [ 0.9286,  0.4095,  1.0091,  ...,  2.6134, -0.7179,  1.9400],\n",
       "        [-1.3079, -0.4623, -0.7608,  ..., -2.1933,  0.1715,  0.9602],\n",
       "        [-1.7361,  1.7099,  0.9369,  ...,  0.7345, -0.6297, -0.3486]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "linear = torch.nn.Linear(784, 10, bias=True).to(device)\n",
    "torch.nn.init.normal_(linear.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. 위에서 구현한 모델을 학습시키기 위해서는 loss 함수와 opitmizer가 필요합니다. 아래 제시된 loss 함수와 optimizer를 구현해보세요. Loss 함수와 optimizer는 모델 안의 가중치를 업데이트 할 때 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 옵티마이저는 SGD, Loss는 Cross Entropy Loss를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. 3번 문제까지 해결하셨다면, 이제 학습을 위한 준비는 거의 끝났다고 볼 수 있습니다. 위 구현 함수들을 이용해 학습 Loop를 구현해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에서 구현한 모델, optimzer, loss fn 등을 이용해 학습을 구현해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [100/600], Loss:1.7296, Accuracy: 72.00%\n",
      "Epoch [1/15], Step [200/600], Loss:0.9654, Accuracy: 78.00%\n",
      "Epoch [1/15], Step [300/600], Loss:0.8391, Accuracy: 83.00%\n",
      "Epoch [1/15], Step [400/600], Loss:1.2242, Accuracy: 75.00%\n",
      "Epoch [1/15], Step [500/600], Loss:1.2674, Accuracy: 79.00%\n",
      "Epoch [1/15], Step [600/600], Loss:0.3690, Accuracy: 93.00%\n",
      "Epoch [2/15], Step [100/600], Loss:1.0288, Accuracy: 78.00%\n",
      "Epoch [2/15], Step [200/600], Loss:0.6694, Accuracy: 84.00%\n",
      "Epoch [2/15], Step [300/600], Loss:0.7971, Accuracy: 82.00%\n",
      "Epoch [2/15], Step [400/600], Loss:0.7487, Accuracy: 87.00%\n",
      "Epoch [2/15], Step [500/600], Loss:0.8489, Accuracy: 86.00%\n",
      "Epoch [2/15], Step [600/600], Loss:0.4941, Accuracy: 91.00%\n",
      "Epoch [3/15], Step [100/600], Loss:1.2166, Accuracy: 79.00%\n",
      "Epoch [3/15], Step [200/600], Loss:0.6136, Accuracy: 86.00%\n",
      "Epoch [3/15], Step [300/600], Loss:1.2625, Accuracy: 76.00%\n",
      "Epoch [3/15], Step [400/600], Loss:0.5604, Accuracy: 87.00%\n",
      "Epoch [3/15], Step [500/600], Loss:0.7780, Accuracy: 90.00%\n",
      "Epoch [3/15], Step [600/600], Loss:0.3136, Accuracy: 90.00%\n",
      "Epoch [4/15], Step [100/600], Loss:0.5469, Accuracy: 87.00%\n",
      "Epoch [4/15], Step [200/600], Loss:0.4950, Accuracy: 90.00%\n",
      "Epoch [4/15], Step [300/600], Loss:0.7407, Accuracy: 83.00%\n",
      "Epoch [4/15], Step [400/600], Loss:0.3322, Accuracy: 84.00%\n",
      "Epoch [4/15], Step [500/600], Loss:0.6164, Accuracy: 88.00%\n",
      "Epoch [4/15], Step [600/600], Loss:0.7281, Accuracy: 81.00%\n",
      "Epoch [5/15], Step [100/600], Loss:0.5688, Accuracy: 85.00%\n",
      "Epoch [5/15], Step [200/600], Loss:0.5886, Accuracy: 85.00%\n",
      "Epoch [5/15], Step [300/600], Loss:0.8731, Accuracy: 82.00%\n",
      "Epoch [5/15], Step [400/600], Loss:0.6327, Accuracy: 86.00%\n",
      "Epoch [5/15], Step [500/600], Loss:1.3438, Accuracy: 81.00%\n",
      "Epoch [5/15], Step [600/600], Loss:0.5459, Accuracy: 88.00%\n",
      "Epoch [6/15], Step [100/600], Loss:0.2789, Accuracy: 94.00%\n",
      "Epoch [6/15], Step [200/600], Loss:0.7382, Accuracy: 90.00%\n",
      "Epoch [6/15], Step [300/600], Loss:0.6071, Accuracy: 83.00%\n",
      "Epoch [6/15], Step [400/600], Loss:0.6295, Accuracy: 87.00%\n",
      "Epoch [6/15], Step [500/600], Loss:0.6942, Accuracy: 85.00%\n",
      "Epoch [6/15], Step [600/600], Loss:0.4904, Accuracy: 89.00%\n",
      "Epoch [7/15], Step [100/600], Loss:0.4508, Accuracy: 94.00%\n",
      "Epoch [7/15], Step [200/600], Loss:0.4767, Accuracy: 89.00%\n",
      "Epoch [7/15], Step [300/600], Loss:0.5495, Accuracy: 88.00%\n",
      "Epoch [7/15], Step [400/600], Loss:0.8242, Accuracy: 81.00%\n",
      "Epoch [7/15], Step [500/600], Loss:0.5807, Accuracy: 86.00%\n",
      "Epoch [7/15], Step [600/600], Loss:0.4373, Accuracy: 91.00%\n",
      "Epoch [8/15], Step [100/600], Loss:0.3018, Accuracy: 90.00%\n",
      "Epoch [8/15], Step [200/600], Loss:0.2331, Accuracy: 92.00%\n",
      "Epoch [8/15], Step [300/600], Loss:0.8823, Accuracy: 87.00%\n",
      "Epoch [8/15], Step [400/600], Loss:0.2254, Accuracy: 92.00%\n",
      "Epoch [8/15], Step [500/600], Loss:0.6467, Accuracy: 86.00%\n",
      "Epoch [8/15], Step [600/600], Loss:0.7355, Accuracy: 87.00%\n",
      "Epoch [9/15], Step [100/600], Loss:0.6975, Accuracy: 87.00%\n",
      "Epoch [9/15], Step [200/600], Loss:0.5358, Accuracy: 83.00%\n",
      "Epoch [9/15], Step [300/600], Loss:0.4903, Accuracy: 88.00%\n",
      "Epoch [9/15], Step [400/600], Loss:0.5187, Accuracy: 88.00%\n",
      "Epoch [9/15], Step [500/600], Loss:0.6276, Accuracy: 90.00%\n",
      "Epoch [9/15], Step [600/600], Loss:0.4526, Accuracy: 91.00%\n",
      "Epoch [10/15], Step [100/600], Loss:0.8184, Accuracy: 85.00%\n",
      "Epoch [10/15], Step [200/600], Loss:0.6985, Accuracy: 82.00%\n",
      "Epoch [10/15], Step [300/600], Loss:0.8617, Accuracy: 84.00%\n",
      "Epoch [10/15], Step [400/600], Loss:0.7611, Accuracy: 89.00%\n",
      "Epoch [10/15], Step [500/600], Loss:0.8983, Accuracy: 85.00%\n",
      "Epoch [10/15], Step [600/600], Loss:0.3075, Accuracy: 88.00%\n",
      "Epoch [11/15], Step [100/600], Loss:0.2244, Accuracy: 91.00%\n",
      "Epoch [11/15], Step [200/600], Loss:0.4829, Accuracy: 91.00%\n",
      "Epoch [11/15], Step [300/600], Loss:0.3528, Accuracy: 85.00%\n",
      "Epoch [11/15], Step [400/600], Loss:0.4363, Accuracy: 89.00%\n",
      "Epoch [11/15], Step [500/600], Loss:0.3906, Accuracy: 89.00%\n",
      "Epoch [11/15], Step [600/600], Loss:0.3768, Accuracy: 91.00%\n",
      "Epoch [12/15], Step [100/600], Loss:0.6345, Accuracy: 86.00%\n",
      "Epoch [12/15], Step [200/600], Loss:0.6942, Accuracy: 86.00%\n",
      "Epoch [12/15], Step [300/600], Loss:0.4978, Accuracy: 83.00%\n",
      "Epoch [12/15], Step [400/600], Loss:0.4605, Accuracy: 93.00%\n",
      "Epoch [12/15], Step [500/600], Loss:0.6150, Accuracy: 88.00%\n",
      "Epoch [12/15], Step [600/600], Loss:0.2540, Accuracy: 91.00%\n",
      "Epoch [13/15], Step [100/600], Loss:0.2330, Accuracy: 94.00%\n",
      "Epoch [13/15], Step [200/600], Loss:0.5524, Accuracy: 88.00%\n",
      "Epoch [13/15], Step [300/600], Loss:0.5680, Accuracy: 89.00%\n",
      "Epoch [13/15], Step [400/600], Loss:0.4461, Accuracy: 87.00%\n",
      "Epoch [13/15], Step [500/600], Loss:0.3226, Accuracy: 94.00%\n",
      "Epoch [13/15], Step [600/600], Loss:0.3142, Accuracy: 91.00%\n",
      "Epoch [14/15], Step [100/600], Loss:0.4843, Accuracy: 90.00%\n",
      "Epoch [14/15], Step [200/600], Loss:0.8362, Accuracy: 82.00%\n",
      "Epoch [14/15], Step [300/600], Loss:0.2432, Accuracy: 92.00%\n",
      "Epoch [14/15], Step [400/600], Loss:0.4936, Accuracy: 90.00%\n",
      "Epoch [14/15], Step [500/600], Loss:0.1223, Accuracy: 95.00%\n",
      "Epoch [14/15], Step [600/600], Loss:0.4192, Accuracy: 92.00%\n",
      "Epoch [15/15], Step [100/600], Loss:0.4614, Accuracy: 90.00%\n",
      "Epoch [15/15], Step [200/600], Loss:0.5346, Accuracy: 85.00%\n",
      "Epoch [15/15], Step [300/600], Loss:0.1843, Accuracy: 94.00%\n",
      "Epoch [15/15], Step [400/600], Loss:0.3109, Accuracy: 89.00%\n",
      "Epoch [15/15], Step [500/600], Loss:0.3329, Accuracy: 92.00%\n",
      "Epoch [15/15], Step [600/600], Loss:0.2550, Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = imgs.view(-1, 28*28)\n",
    "        \n",
    "        outputs = linear(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, argmax = torch.max(outputs, 1)\n",
    "        accuracy = (labels == argmax).float().mean()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss:{:.4f}, Accuracy: {:.2f}%'.format( epoch+1, training_epochs,\n",
    "            i+1, len(train_loader), loss.item(), accuracy.item()* 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. 학습이 완료되면, 모델이 잘 동작하는지 테스트가 필요합니다. 데이터로드 파트에서 준비했던 테스트 데이터를 이용해 테스트를 진행해봅시다. 아래 테스트 코드를 완성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for 100 images: 63.00%\n",
      "Test accuracy for 200 images: 63.00%\n",
      "Test accuracy for 300 images: 62.33%\n",
      "Test accuracy for 400 images: 63.00%\n",
      "Test accuracy for 500 images: 63.80%\n",
      "Test accuracy for 600 images: 61.83%\n",
      "Test accuracy for 700 images: 62.14%\n",
      "Test accuracy for 800 images: 63.00%\n",
      "Test accuracy for 900 images: 63.22%\n",
      "Test accuracy for 1000 images: 63.30%\n",
      "Test accuracy for 1100 images: 64.36%\n",
      "Test accuracy for 1200 images: 64.67%\n",
      "Test accuracy for 1300 images: 65.31%\n",
      "Test accuracy for 1400 images: 65.43%\n",
      "Test accuracy for 1500 images: 65.67%\n",
      "Test accuracy for 1600 images: 65.56%\n",
      "Test accuracy for 1700 images: 65.53%\n",
      "Test accuracy for 1800 images: 65.17%\n",
      "Test accuracy for 1900 images: 65.47%\n",
      "Test accuracy for 2000 images: 65.70%\n",
      "Test accuracy for 2100 images: 66.00%\n",
      "Test accuracy for 2200 images: 66.18%\n",
      "Test accuracy for 2300 images: 65.83%\n",
      "Test accuracy for 2400 images: 66.12%\n",
      "Test accuracy for 2500 images: 66.16%\n",
      "Test accuracy for 2600 images: 66.46%\n",
      "Test accuracy for 2700 images: 66.48%\n",
      "Test accuracy for 2800 images: 66.36%\n",
      "Test accuracy for 2900 images: 66.45%\n",
      "Test accuracy for 3000 images: 66.53%\n",
      "Test accuracy for 3100 images: 66.55%\n",
      "Test accuracy for 3200 images: 66.56%\n",
      "Test accuracy for 3300 images: 66.58%\n",
      "Test accuracy for 3400 images: 66.29%\n",
      "Test accuracy for 3500 images: 66.40%\n",
      "Test accuracy for 3600 images: 66.47%\n",
      "Test accuracy for 3700 images: 66.59%\n",
      "Test accuracy for 3800 images: 66.79%\n",
      "Test accuracy for 3900 images: 66.51%\n",
      "Test accuracy for 4000 images: 66.45%\n",
      "Test accuracy for 4100 images: 66.24%\n",
      "Test accuracy for 4200 images: 66.17%\n",
      "Test accuracy for 4300 images: 66.37%\n",
      "Test accuracy for 4400 images: 66.52%\n",
      "Test accuracy for 4500 images: 66.29%\n",
      "Test accuracy for 4600 images: 66.33%\n",
      "Test accuracy for 4700 images: 66.34%\n",
      "Test accuracy for 4800 images: 66.25%\n",
      "Test accuracy for 4900 images: 66.33%\n",
      "Test accuracy for 5000 images: 66.36%\n",
      "Test accuracy for 5100 images: 66.31%\n",
      "Test accuracy for 5200 images: 66.23%\n",
      "Test accuracy for 5300 images: 66.15%\n",
      "Test accuracy for 5400 images: 66.17%\n",
      "Test accuracy for 5500 images: 66.07%\n",
      "Test accuracy for 5600 images: 66.11%\n",
      "Test accuracy for 5700 images: 66.21%\n",
      "Test accuracy for 5800 images: 66.17%\n",
      "Test accuracy for 5900 images: 66.19%\n",
      "Test accuracy for 6000 images: 66.30%\n",
      "Test accuracy for 6100 images: 66.30%\n",
      "Test accuracy for 6200 images: 66.35%\n",
      "Test accuracy for 6300 images: 66.35%\n",
      "Test accuracy for 6400 images: 66.34%\n",
      "Test accuracy for 6500 images: 66.29%\n",
      "Test accuracy for 6600 images: 66.24%\n",
      "Test accuracy for 6700 images: 66.12%\n",
      "Test accuracy for 6800 images: 66.12%\n",
      "Test accuracy for 6900 images: 66.09%\n",
      "Test accuracy for 7000 images: 66.20%\n",
      "Test accuracy for 7100 images: 66.15%\n",
      "Test accuracy for 7200 images: 66.18%\n",
      "Test accuracy for 7300 images: 66.14%\n",
      "Test accuracy for 7400 images: 66.19%\n",
      "Test accuracy for 7500 images: 66.25%\n",
      "Test accuracy for 7600 images: 66.29%\n",
      "Test accuracy for 7700 images: 66.34%\n",
      "Test accuracy for 7800 images: 66.32%\n",
      "Test accuracy for 7900 images: 66.37%\n",
      "Test accuracy for 8000 images: 66.34%\n",
      "Test accuracy for 8100 images: 66.31%\n",
      "Test accuracy for 8200 images: 66.35%\n",
      "Test accuracy for 8300 images: 66.31%\n",
      "Test accuracy for 8400 images: 66.36%\n",
      "Test accuracy for 8500 images: 66.39%\n",
      "Test accuracy for 8600 images: 66.33%\n",
      "Test accuracy for 8700 images: 66.33%\n",
      "Test accuracy for 8800 images: 66.33%\n",
      "Test accuracy for 8900 images: 66.24%\n",
      "Test accuracy for 9000 images: 66.20%\n",
      "Test accuracy for 9100 images: 66.25%\n",
      "Test accuracy for 9200 images: 66.24%\n",
      "Test accuracy for 9300 images: 66.25%\n",
      "Test accuracy for 9400 images: 66.28%\n",
      "Test accuracy for 9500 images: 66.25%\n",
      "Test accuracy for 9600 images: 66.21%\n",
      "Test accuracy for 9700 images: 66.16%\n",
      "Test accuracy for 9800 images: 66.13%\n",
      "Test accuracy for 9900 images: 66.16%\n",
      "Test accuracy for 10000 images: 66.21%\n"
     ]
    }
   ],
   "source": [
    "linear.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (imgs, labels) in enumerate(test_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        imgs = imgs.view(-1, 28*28)\n",
    "        outputs = linear(imgs)\n",
    "        _, argmax = torch.max(outputs, 1) #max()를 통해 최종 출력이 가장 높은 class 선택\n",
    "        total += imgs.size(0)\n",
    "        correct += (labels == argmax).sum().item()\n",
    "        \n",
    "        print('Test accuracy for {} images: {:.2f}%'.format(total, correct/total*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
